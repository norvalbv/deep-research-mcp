{
  "matrix": {
    "timestamp": "2025-12-20T04:23:05.079Z",
    "totalComparisons": 35,
    "byCategory": [
      {
        "category": "single_hop_factual",
        "totalSamples": 10,
        "mcpWins": 6,
        "perplexityWins": 2,
        "ties": 2,
        "mcpWinRate": 0.6,
        "bootstrap": {
          "lower": -0.8,
          "upper": 1.3,
          "mean": 0.25100499999999704,
          "isSignificant": false,
          "pSuperiority": 0.6371
        },
        "recommendation": "TIE"
      },
      {
        "category": "multi_hop_reasoning",
        "totalSamples": 10,
        "mcpWins": 2,
        "perplexityWins": 6,
        "ties": 2,
        "mcpWinRate": 0.2,
        "bootstrap": {
          "lower": -2.25,
          "upper": -0.35,
          "mean": -1.3023349999999776,
          "isSignificant": true,
          "pSuperiority": 0.006
        },
        "recommendation": "USE_PERPLEXITY"
      },
      {
        "category": "synthesis",
        "totalSamples": 10,
        "mcpWins": 1,
        "perplexityWins": 3,
        "ties": 6,
        "mcpWinRate": 0.1,
        "bootstrap": {
          "lower": -1.8,
          "upper": -0.2,
          "mean": -1.0038449999999783,
          "isSignificant": true,
          "pSuperiority": 0
        },
        "recommendation": "USE_PERPLEXITY"
      },
      {
        "category": "code_generation",
        "totalSamples": 5,
        "mcpWins": 0,
        "perplexityWins": 4,
        "ties": 1,
        "mcpWinRate": 0,
        "bootstrap": {
          "lower": -3.6,
          "upper": -1.3,
          "mean": -2.605200000000003,
          "isSignificant": true,
          "pSuperiority": 0
        },
        "recommendation": "USE_PERPLEXITY"
      }
    ],
    "switchingPoints": [],
    "summary": {
      "mcpStrongCategories": [],
      "perplexityStrongCategories": [
        "multi_hop_reasoning",
        "synthesis",
        "code_generation"
      ],
      "tieCategories": [
        "single_hop_factual"
      ]
    }
  },
  "results": [
    {
      "sample": {
        "id": "shf-01",
        "category": "single_hop_factual",
        "query": "What is the context window size of GPT-4 Turbo?",
        "goldStandard": {
          "answer": "GPT-4 Turbo has a 128K token context window.",
          "atomicFacts": [
            "GPT-4 Turbo context window is 128K tokens"
          ],
          "sources": [
            "OpenAI documentation"
          ]
        },
        "expectedWinner": "perplexity",
        "rationale": "Simple factual lookup favors faster systems"
      },
      "winner": "baseline",
      "systemScore": 3,
      "baselineScore": 5
    },
    {
      "sample": {
        "id": "shf-02",
        "category": "single_hop_factual",
        "query": "When was the Transformer architecture paper published?",
        "goldStandard": {
          "answer": "The 'Attention Is All You Need' paper was published in June 2017.",
          "atomicFacts": [
            "Transformer paper published June 2017"
          ],
          "sources": [
            "arxiv:1706.03762"
          ]
        },
        "expectedWinner": "perplexity",
        "rationale": "Direct retrieval task"
      },
      "winner": "baseline",
      "systemScore": 2.5,
      "baselineScore": 5
    },
    {
      "sample": {
        "id": "shf-03",
        "category": "single_hop_factual",
        "query": "What is the default temperature setting for OpenAI's API?",
        "goldStandard": {
          "answer": "The default temperature is 1.0.",
          "atomicFacts": [
            "OpenAI API default temperature is 1.0"
          ],
          "sources": [
            "OpenAI API docs"
          ]
        },
        "expectedWinner": "perplexity",
        "rationale": "Simple API documentation lookup"
      },
      "winner": "system",
      "systemScore": 5,
      "baselineScore": 4
    },
    {
      "sample": {
        "id": "shf-04",
        "category": "single_hop_factual",
        "query": "How many parameters does Llama 2 70B have?",
        "goldStandard": {
          "answer": "Llama 2 70B has approximately 70 billion parameters.",
          "atomicFacts": [
            "Llama 2 70B has 70 billion parameters"
          ],
          "sources": [
            "Meta AI"
          ]
        },
        "expectedWinner": "perplexity",
        "rationale": "Direct factual retrieval"
      },
      "winner": "tie",
      "systemScore": 4,
      "baselineScore": 4.5
    },
    {
      "sample": {
        "id": "shf-05",
        "category": "single_hop_factual",
        "query": "What programming language is PyTorch primarily written in?",
        "goldStandard": {
          "answer": "PyTorch is primarily written in Python and C++.",
          "atomicFacts": [
            "PyTorch written in Python",
            "PyTorch written in C++"
          ],
          "sources": [
            "PyTorch GitHub"
          ]
        },
        "expectedWinner": "perplexity",
        "rationale": "Simple technical fact"
      },
      "winner": "tie",
      "systemScore": 3.5,
      "baselineScore": 4.5
    },
    {
      "sample": {
        "id": "shf-06",
        "category": "single_hop_factual",
        "query": "What is the maximum batch size supported by Anthropic's Claude API?",
        "goldStandard": {
          "answer": "Claude's batch API supports up to 10,000 requests per batch.",
          "atomicFacts": [
            "Claude batch API supports 10,000 requests per batch"
          ],
          "sources": [
            "Anthropic API docs"
          ]
        },
        "expectedWinner": "perplexity",
        "rationale": "API specification lookup"
      },
      "winner": "system",
      "systemScore": 5,
      "baselineScore": 4
    },
    {
      "sample": {
        "id": "shf-07",
        "category": "single_hop_factual",
        "query": "What year did BERT achieve state-of-the-art on GLUE?",
        "goldStandard": {
          "answer": "BERT achieved state-of-the-art on GLUE in 2018.",
          "atomicFacts": [
            "BERT SOTA on GLUE in 2018"
          ],
          "sources": [
            "arxiv:1810.04805"
          ]
        },
        "expectedWinner": "perplexity",
        "rationale": "Historical fact retrieval"
      },
      "winner": "system",
      "systemScore": 5,
      "baselineScore": 4
    },
    {
      "sample": {
        "id": "shf-08",
        "category": "single_hop_factual",
        "query": "What is the embedding dimension of text-embedding-3-large?",
        "goldStandard": {
          "answer": "text-embedding-3-large has 3072 dimensions.",
          "atomicFacts": [
            "text-embedding-3-large has 3072 dimensions"
          ],
          "sources": [
            "OpenAI embeddings docs"
          ]
        },
        "expectedWinner": "perplexity",
        "rationale": "Technical specification"
      },
      "winner": "system",
      "systemScore": 5,
      "baselineScore": 4
    },
    {
      "sample": {
        "id": "shf-09",
        "category": "single_hop_factual",
        "query": "What is the default chunk size in LangChain's RecursiveCharacterTextSplitter?",
        "goldStandard": {
          "answer": "The default chunk_size is 4000 characters.",
          "atomicFacts": [
            "LangChain RecursiveCharacterTextSplitter default chunk_size is 4000"
          ],
          "sources": [
            "LangChain docs"
          ]
        },
        "expectedWinner": "perplexity",
        "rationale": "Library documentation lookup"
      },
      "winner": "system",
      "systemScore": 4.5,
      "baselineScore": 1
    },
    {
      "sample": {
        "id": "shf-10",
        "category": "single_hop_factual",
        "query": "What activation function does GPT use?",
        "goldStandard": {
          "answer": "GPT uses GELU (Gaussian Error Linear Unit) activation.",
          "atomicFacts": [
            "GPT uses GELU activation"
          ],
          "sources": [
            "GPT paper"
          ]
        },
        "expectedWinner": "perplexity",
        "rationale": "Architecture detail"
      },
      "winner": "system",
      "systemScore": 5,
      "baselineScore": 4
    },
    {
      "sample": {
        "id": "mhr-01",
        "category": "multi_hop_reasoning",
        "query": "What embedding model should I use if I need the same dimensionality as BERT but better performance on retrieval tasks?",
        "goldStandard": {
          "answer": "Use text-embedding-3-small with dimensions=768. BERT uses 768 dimensions, and text-embedding-3-small can be configured to 768 dims while outperforming BERT on MTEB retrieval benchmarks.",
          "atomicFacts": [
            "BERT uses 768 dimensions",
            "text-embedding-3-small supports 768 dimensions",
            "text-embedding-3-small outperforms BERT on MTEB retrieval"
          ],
          "sources": [
            "OpenAI docs",
            "MTEB leaderboard"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Requires connecting BERT specs with embedding model capabilities and benchmarks"
      },
      "winner": "system",
      "systemScore": 4.5,
      "baselineScore": 3.5
    },
    {
      "sample": {
        "id": "mhr-02",
        "category": "multi_hop_reasoning",
        "query": "If I'm building a RAG system that needs to handle 50K token documents, which combination of embedding model and LLM would minimize costs while maintaining quality?",
        "goldStandard": {
          "answer": "Use text-embedding-3-small ($0.02/1M tokens) for embeddings with chunking, paired with GPT-4o-mini for generation. GPT-4o-mini supports 128K context at $0.15/1M input tokens, making it 20x cheaper than GPT-4 Turbo for long documents.",
          "atomicFacts": [
            "text-embedding-3-small costs $0.02/1M tokens",
            "GPT-4o-mini supports 128K context",
            "GPT-4o-mini costs $0.15/1M input tokens",
            "GPT-4o-mini is 20x cheaper than GPT-4 Turbo"
          ],
          "sources": [
            "OpenAI pricing",
            "Model specs"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Multi-hop: document size -> context requirements -> cost optimization"
      },
      "winner": "tie",
      "systemScore": 3.5,
      "baselineScore": 3.5
    },
    {
      "sample": {
        "id": "mhr-03",
        "category": "multi_hop_reasoning",
        "query": "What is the maximum effective context length for Llama 3 when using RoPE scaling, and how does this compare to Claude 3?",
        "goldStandard": {
          "answer": "Llama 3 can extend to ~65K tokens with RoPE scaling (base 8K x 8). Claude 3 Opus natively supports 200K tokens without scaling, making Claude better for very long contexts while Llama 3 with RoPE is more cost-effective for 32-65K range.",
          "atomicFacts": [
            "Llama 3 base context is 8K tokens",
            "RoPE scaling extends Llama 3 to ~65K tokens",
            "Claude 3 Opus supports 200K tokens natively",
            "Claude better for >65K contexts"
          ],
          "sources": [
            "Meta AI",
            "Anthropic docs",
            "RoPE paper"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Requires connecting RoPE mechanics, Llama specs, and Claude comparison"
      },
      "winner": "tie",
      "systemScore": 4,
      "baselineScore": 4.5
    },
    {
      "sample": {
        "id": "mhr-04",
        "category": "multi_hop_reasoning",
        "query": "For a financial document QA system requiring 99.9% factual accuracy, what combination of retrieval and verification approaches should I use?",
        "goldStandard": {
          "answer": "Use hybrid retrieval (BM25 + dense embeddings) with a multi-stage verification pipeline: 1) NLI verification using DeBERTa-v3 for each claim, 2) Citation grounding check, 3) Numeric consistency validation. Target CCR >95% and Citation Fidelity >99%.",
          "atomicFacts": [
            "Use hybrid retrieval (BM25 + dense)",
            "Use DeBERTa-v3 for NLI verification",
            "Implement citation grounding check",
            "Target CCR >95%",
            "Target Citation Fidelity >99%"
          ],
          "sources": [
            "arxiv:2309.15217",
            "arxiv:2203.05115"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Complex multi-hop: accuracy requirement -> retrieval strategy -> verification pipeline"
      },
      "winner": "baseline",
      "systemScore": 3.5,
      "baselineScore": 5
    },
    {
      "sample": {
        "id": "mhr-05",
        "category": "multi_hop_reasoning",
        "query": "What chunking strategy should I use for technical documentation if my embedding model has 512 token limit but I need to preserve code block context?",
        "goldStandard": {
          "answer": "Use semantic chunking with code-aware splitting: 1) Detect code blocks and treat as atomic units, 2) Use RecursiveCharacterTextSplitter with separators prioritizing markdown headers and code fences, 3) Set chunk_size=400, overlap=50 to stay under 512 limit with buffer for special tokens.",
          "atomicFacts": [
            "Use semantic chunking",
            "Treat code blocks as atomic units",
            "Use RecursiveCharacterTextSplitter",
            "Prioritize markdown headers and code fences",
            "chunk_size=400 with overlap=50"
          ],
          "sources": [
            "LangChain docs",
            "RAG best practices"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Multi-hop: embedding limit -> code preservation -> chunking strategy"
      },
      "winner": "baseline",
      "systemScore": 2,
      "baselineScore": 4
    },
    {
      "sample": {
        "id": "mhr-06",
        "category": "multi_hop_reasoning",
        "query": "If fine-tuning GPT-4o costs $25/1M training tokens and I have 10K examples of 500 tokens each, what's the minimum epochs needed to see improvement on instruction following, and what would it cost?",
        "goldStandard": {
          "answer": "10K examples x 500 tokens = 5M tokens. Research suggests 2-3 epochs for instruction following tasks. Cost: 5M x 3 epochs x $25/1M = $375. However, consider that GPT-4o may already perform well on instruction following, so validate with IFEval benchmark first.",
          "atomicFacts": [
            "10K x 500 = 5M training tokens",
            "2-3 epochs needed for instruction following",
            "Cost is ~$375 for 3 epochs",
            "Validate with IFEval benchmark first"
          ],
          "sources": [
            "OpenAI fine-tuning docs",
            "arxiv:2311.07911"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Multi-hop: training data size -> epoch requirements -> cost calculation"
      },
      "winner": "system",
      "systemScore": 5,
      "baselineScore": 4
    },
    {
      "sample": {
        "id": "mhr-07",
        "category": "multi_hop_reasoning",
        "query": "What's the tradeoff between using a reranker vs just increasing top-k in retrieval for a latency-sensitive application?",
        "goldStandard": {
          "answer": "Increasing top-k from 5 to 20 adds ~10-20ms latency but may include noise. A cross-encoder reranker like ms-marco-MiniLM adds ~50-100ms but improves precision by 15-20%. For latency <200ms target: use top-k=10 without reranker. For accuracy-critical: use top-k=20 with reranker, accept ~150ms total.",
          "atomicFacts": [
            "Increasing top-k adds 10-20ms latency",
            "Higher top-k may include noise",
            "Cross-encoder reranker adds 50-100ms",
            "Reranker improves precision by 15-20%",
            "Latency target <200ms: skip reranker"
          ],
          "sources": [
            "BEIR benchmark",
            "Sentence-transformers docs"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Multi-hop: latency constraints -> retrieval options -> accuracy tradeoffs"
      },
      "winner": "baseline",
      "systemScore": 1,
      "baselineScore": 4.5
    },
    {
      "sample": {
        "id": "mhr-08",
        "category": "multi_hop_reasoning",
        "query": "How should I implement rate limiting for an LLM API that has both RPM and TPM limits, when my requests vary from 100 to 10K tokens?",
        "goldStandard": {
          "answer": "Implement dual token bucket algorithm: 1) RPM bucket with capacity=limit, refill=limit/60 per second, 2) TPM bucket with capacity=limit, refill=limit/60 per second. Before each request, check both buckets: wait if RPM bucket <1 OR TPM bucket < estimated_tokens. Use exponential backoff (2^n seconds, max 32s) on 429 errors.",
          "atomicFacts": [
            "Use dual token bucket algorithm",
            "RPM bucket refills at limit/60 per second",
            "TPM bucket refills at limit/60 per second",
            "Check both buckets before each request",
            "Use exponential backoff on 429"
          ],
          "sources": [
            "OpenAI rate limits",
            "Token bucket algorithm"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Multi-hop: variable request sizes -> dual limits -> implementation strategy"
      },
      "winner": "baseline",
      "systemScore": 2.5,
      "baselineScore": 4.5
    },
    {
      "sample": {
        "id": "mhr-09",
        "category": "multi_hop_reasoning",
        "query": "What's the optimal batch size for processing 1M documents through an embedding API with 3M TPM limit and 5K RPM limit?",
        "goldStandard": {
          "answer": "Assuming avg 200 tokens/doc: 3M TPM / 200 = 15K docs/min from TPM, but 5K RPM caps at 5K docs/min. Optimal batch size: 3M / 5K = 600 tokens/request = 3 docs/batch. Total time: 1M / 5K RPM = 200 minutes. Consider batching 3 docs per request to maximize throughput within both limits.",
          "atomicFacts": [
            "TPM limit allows 15K docs/min at 200 tokens/doc",
            "RPM limit caps at 5K requests/min",
            "Optimal batch is 3 docs per request",
            "Total processing time ~200 minutes"
          ],
          "sources": [
            "API limits calculation",
            "Batch optimization"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Multi-hop: document count -> dual limits -> batch optimization"
      },
      "winner": "baseline",
      "systemScore": 2,
      "baselineScore": 4.5
    },
    {
      "sample": {
        "id": "mhr-10",
        "category": "multi_hop_reasoning",
        "query": "If I need to maintain conversation history for 100 concurrent users with 20K token average context, what's the most cost-effective caching strategy?",
        "goldStandard": {
          "answer": "100 users x 20K tokens = 2M tokens active context. Options: 1) Redis with msgpack serialization (~$50/month for 2GB), 2) KV cache in Cloudflare Workers ($5/10M reads). For LLM, use prompt caching (50% discount on cached tokens). Implement sliding window to keep only last 16K tokens, summarize older context.",
          "atomicFacts": [
            "Total active context is 2M tokens",
            "Redis option ~$50/month",
            "Cloudflare KV option ~$5/10M reads",
            "Prompt caching gives 50% discount",
            "Use sliding window with 16K limit"
          ],
          "sources": [
            "Redis pricing",
            "Cloudflare pricing",
            "OpenAI prompt caching"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Multi-hop: user count -> memory requirements -> caching strategy -> cost"
      },
      "winner": "baseline",
      "systemScore": 2,
      "baselineScore": 5
    },
    {
      "sample": {
        "id": "syn-01",
        "category": "synthesis",
        "query": "Compare the major approaches to RAG evaluation and recommend which metrics to use for a production system.",
        "goldStandard": {
          "answer": "Three main approaches: 1) RAGAS (context relevance, faithfulness, answer relevance) - good for automated testing, 2) RGB framework (noise robustness, negative rejection, counterfactual) - tests edge cases, 3) ARES (LLM-as-judge with confidence calibration) - correlates with human judgment. For production: use RAGAS for CI/CD, RGB for regression testing, human eval sample for calibration. Target: Faithfulness >0.9, Context Relevance >0.8.",
          "atomicFacts": [
            "RAGAS measures context relevance, faithfulness, answer relevance",
            "RGB tests noise robustness, negative rejection, counterfactual",
            "ARES uses LLM-as-judge with confidence calibration",
            "Use RAGAS for CI/CD",
            "Use RGB for regression testing",
            "Target Faithfulness >0.9"
          ],
          "sources": [
            "arxiv:2309.15217",
            "arxiv:2309.01431",
            "arxiv:2311.09476"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Synthesis across multiple evaluation frameworks"
      },
      "winner": "tie",
      "systemScore": 3,
      "baselineScore": 4.5
    },
    {
      "sample": {
        "id": "syn-02",
        "category": "synthesis",
        "query": "What are the current best practices for LLM output validation in production systems?",
        "goldStandard": {
          "answer": "Multi-layer validation: 1) Structural - JSON schema validation, regex for format, 2) Semantic - NLI entailment check against sources (DeBERTa-v3), fact extraction and verification, 3) Safety - content filtering, PII detection, 4) Consistency - PVR (Parallel-Verify-Resolve) for multi-source synthesis. Implement as pipeline with early termination on critical failures. Log all validation results for continuous improvement.",
          "atomicFacts": [
            "Structural validation includes JSON schema and regex",
            "Semantic validation uses NLI entailment",
            "Use DeBERTa-v3 for NLI",
            "Safety layer includes PII detection",
            "PVR for consistency checking",
            "Early termination on critical failures"
          ],
          "sources": [
            "Production ML practices",
            "arxiv:2203.05115"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Synthesis of validation approaches across multiple concerns"
      },
      "winner": "baseline",
      "systemScore": 2,
      "baselineScore": 4.5
    },
    {
      "sample": {
        "id": "syn-03",
        "category": "synthesis",
        "query": "Summarize the evolution of context length handling in LLMs from GPT-3 to current models.",
        "goldStandard": {
          "answer": "Evolution: GPT-3 (2K) -> GPT-3.5 (4K, then 16K) -> GPT-4 (8K, then 32K, then 128K Turbo) -> Claude (100K -> 200K) -> Gemini (1M). Key innovations: 1) RoPE (rotary embeddings) enabled extrapolation, 2) ALiBi for length generalization, 3) Sliding window attention (Mistral), 4) Ring attention for distributed long context. Tradeoff: longer context = higher cost and potential attention dilution.",
          "atomicFacts": [
            "GPT-3 had 2K context",
            "GPT-4 Turbo has 128K context",
            "Claude 3 has 200K context",
            "Gemini has 1M context",
            "RoPE enables extrapolation",
            "Longer context increases cost"
          ],
          "sources": [
            "Model release notes",
            "arxiv:2104.09864",
            "arxiv:2308.16137"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Historical synthesis across model generations"
      },
      "winner": "tie",
      "systemScore": 3.5,
      "baselineScore": 4
    },
    {
      "sample": {
        "id": "syn-04",
        "category": "synthesis",
        "query": "What are the tradeoffs between different vector database options for RAG systems?",
        "goldStandard": {
          "answer": "Options analysis: 1) Pinecone - managed, serverless, $70/1M vectors, best for startups, 2) Weaviate - hybrid search built-in, good for complex queries, 3) Qdrant - best performance/cost ratio, self-hosted or cloud, 4) pgvector - use existing Postgres, good for <1M vectors, 5) Chroma - dev-friendly, in-process. Decision matrix: <100K vectors -> pgvector or Chroma, 100K-10M -> Qdrant or Weaviate, >10M -> Pinecone or Qdrant distributed.",
          "atomicFacts": [
            "Pinecone costs ~$70/1M vectors",
            "Weaviate has built-in hybrid search",
            "Qdrant has best performance/cost ratio",
            "pgvector good for <1M vectors",
            "Chroma is dev-friendly",
            "<100K vectors use pgvector or Chroma"
          ],
          "sources": [
            "Vector DB benchmarks",
            "Pricing pages"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Synthesis of multiple database options with tradeoffs"
      },
      "winner": "system",
      "systemScore": 5,
      "baselineScore": 3.5
    },
    {
      "sample": {
        "id": "syn-05",
        "category": "synthesis",
        "query": "What are the key findings from recent research on reducing LLM hallucinations?",
        "goldStandard": {
          "answer": "Key findings: 1) Retrieval grounding reduces hallucinations by 40-60% (RAG), 2) Chain-of-verification (CoVe) detects self-inconsistencies, 3) Constrained decoding limits vocabulary to source terms, 4) Self-consistency (sample multiple, vote) improves factuality, 5) Fine-tuning on verified facts helps but doesn't eliminate issue. Most effective: combine RAG + CoVe + citation requirements. Open problem: models still confident when wrong.",
          "atomicFacts": [
            "RAG reduces hallucinations 40-60%",
            "Chain-of-verification detects self-inconsistencies",
            "Constrained decoding limits to source terms",
            "Self-consistency improves factuality",
            "Combine RAG + CoVe + citations",
            "Models remain confident when wrong"
          ],
          "sources": [
            "arxiv:2309.11495",
            "arxiv:2212.09561",
            "arxiv:2203.11171"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Research synthesis across hallucination mitigation techniques"
      },
      "winner": "tie",
      "systemScore": 3.5,
      "baselineScore": 4
    },
    {
      "sample": {
        "id": "syn-06",
        "category": "synthesis",
        "query": "Compare instruction tuning methods and when to use each.",
        "goldStandard": {
          "answer": "Methods: 1) SFT (Supervised Fine-Tuning) - basic, needs 1K-10K examples, 2) RLHF - aligns with human preferences, expensive, 3) DPO (Direct Preference Optimization) - simpler than RLHF, similar results, 4) ORPO - combines SFT+preference in one step. Use cases: SFT for format/style, DPO for preference alignment, RLHF for safety-critical. Cost: SFT < DPO < ORPO < RLHF. Recommendation: Start with SFT, add DPO if preference matters.",
          "atomicFacts": [
            "SFT needs 1K-10K examples",
            "RLHF is expensive but effective",
            "DPO is simpler than RLHF with similar results",
            "ORPO combines SFT and preference",
            "Use SFT for format/style",
            "Start with SFT then add DPO"
          ],
          "sources": [
            "arxiv:2305.18290",
            "arxiv:2402.01306",
            "arxiv:2310.12036"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Synthesis of instruction tuning approaches"
      },
      "winner": "tie",
      "systemScore": 3.5,
      "baselineScore": 3.5
    },
    {
      "sample": {
        "id": "syn-07",
        "category": "synthesis",
        "query": "What monitoring and observability should be implemented for production LLM applications?",
        "goldStandard": {
          "answer": "Layers: 1) Operational - latency p50/p95/p99, token usage, error rates, cost tracking, 2) Quality - output validation pass rate, hallucination detection, user feedback correlation, 3) Drift - embedding drift detection, prompt template effectiveness over time, 4) Security - PII leakage monitoring, prompt injection attempts. Tools: LangSmith for tracing, custom dashboards for metrics. Alert on: latency >2x baseline, error rate >1%, cost anomalies >20%.",
          "atomicFacts": [
            "Track latency p50/p95/p99",
            "Monitor token usage and cost",
            "Track output validation pass rate",
            "Detect embedding drift",
            "Monitor PII leakage",
            "Use LangSmith for tracing",
            "Alert on latency >2x baseline"
          ],
          "sources": [
            "MLOps best practices",
            "LangSmith docs"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Synthesis of observability concerns"
      },
      "winner": "baseline",
      "systemScore": 2.5,
      "baselineScore": 5
    },
    {
      "sample": {
        "id": "syn-08",
        "category": "synthesis",
        "query": "What are the emerging patterns for multi-agent LLM systems?",
        "goldStandard": {
          "answer": "Patterns: 1) Supervisor - central coordinator delegates to specialists, 2) Debate - agents argue, judge decides, improves reasoning, 3) Workflow - DAG of specialized agents, 4) Swarm - emergent coordination, less predictable. Frameworks: AutoGen (Microsoft), CrewAI, LangGraph. Challenges: cost multiplication (N agents = Nx cost), coordination overhead, debugging complexity. Best practice: start with 2-3 agents, add only if measurable improvement.",
          "atomicFacts": [
            "Supervisor pattern uses central coordinator",
            "Debate pattern improves reasoning",
            "Workflow uses DAG of specialists",
            "Swarm has emergent coordination",
            "AutoGen, CrewAI, LangGraph are frameworks",
            "N agents = Nx cost",
            "Start with 2-3 agents"
          ],
          "sources": [
            "arxiv:2308.08155",
            "Framework docs"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Synthesis of multi-agent patterns"
      },
      "winner": "tie",
      "systemScore": 2.5,
      "baselineScore": 3
    },
    {
      "sample": {
        "id": "syn-09",
        "category": "synthesis",
        "query": "What are the best practices for prompt engineering in 2024?",
        "goldStandard": {
          "answer": "Key practices: 1) Structured output (JSON mode, function calling) over free-form, 2) Few-shot > zero-shot for complex tasks (3-5 examples optimal), 3) Chain-of-thought for reasoning tasks, 4) System prompts for persistent behavior, 5) XML/markdown tags for section delineation. Anti-patterns: excessive length (diminishing returns >2K tokens), conflicting instructions, vague success criteria. Testing: maintain prompt test suites, version control prompts.",
          "atomicFacts": [
            "Use structured output over free-form",
            "Few-shot with 3-5 examples optimal",
            "Chain-of-thought for reasoning",
            "Use system prompts for persistent behavior",
            "XML/markdown tags for sections",
            "Diminishing returns >2K tokens",
            "Version control prompts"
          ],
          "sources": [
            "Anthropic prompt guide",
            "OpenAI best practices"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Synthesis of prompt engineering practices"
      },
      "winner": "tie",
      "systemScore": 3.5,
      "baselineScore": 4
    },
    {
      "sample": {
        "id": "syn-10",
        "category": "synthesis",
        "query": "Compare the major LLM API providers on reliability and uptime.",
        "goldStandard": {
          "answer": "Comparison: OpenAI - 99.9% SLA (enterprise), occasional capacity issues during peaks, best model variety. Anthropic - no public SLA, generally stable, slower to add features. Google (Vertex AI) - 99.9% SLA, enterprise-grade, complex pricing. AWS Bedrock - 99.9% SLA, multi-provider access, higher latency. Recommendation: primary + fallback strategy (e.g., OpenAI primary, Anthropic fallback). Monitor via status pages and synthetic checks.",
          "atomicFacts": [
            "OpenAI has 99.9% SLA for enterprise",
            "OpenAI has occasional capacity issues",
            "Anthropic has no public SLA",
            "Vertex AI has 99.9% SLA",
            "AWS Bedrock has higher latency",
            "Use primary + fallback strategy"
          ],
          "sources": [
            "Provider SLA pages",
            "Status page history"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Synthesis of provider comparison"
      },
      "winner": "baseline",
      "systemScore": 2,
      "baselineScore": 5
    },
    {
      "sample": {
        "id": "cg-01",
        "category": "code_generation",
        "query": "Write a Python function to implement exponential backoff with jitter for API retries.",
        "goldStandard": {
          "answer": "```python\nimport random\nimport time\nfrom typing import TypeVar, Callable\n\nT = TypeVar('T')\n\ndef retry_with_backoff(\n    func: Callable[[], T],\n    max_retries: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 32.0,\n    jitter: bool = True\n) -> T:\n    for attempt in range(max_retries + 1):\n        try:\n            return func()\n        except Exception as e:\n            if attempt == max_retries:\n                raise\n            delay = min(base_delay * (2 ** attempt), max_delay)\n            if jitter:\n                delay *= (0.5 + random.random())\n            time.sleep(delay)\n```",
          "atomicFacts": [
            "Uses exponential backoff formula",
            "Includes jitter option",
            "Has max_delay cap",
            "Generic return type",
            "Raises on final failure"
          ],
          "sources": [
            "AWS retry best practices"
          ]
        },
        "expectedWinner": "tie",
        "rationale": "Both can generate quality code for standard patterns"
      },
      "winner": "tie",
      "systemScore": 0,
      "baselineScore": 0
    },
    {
      "sample": {
        "id": "cg-02",
        "category": "code_generation",
        "query": "Write TypeScript code to implement a token bucket rate limiter for LLM API calls.",
        "goldStandard": {
          "answer": "```typescript\nclass TokenBucket {\n  private tokens: number;\n  private lastRefill: number;\n  constructor(\n    private capacity: number,\n    private refillRate: number // tokens per second\n  ) {\n    this.tokens = capacity;\n    this.lastRefill = Date.now();\n  }\n  \n  async acquire(cost: number = 1): Promise<void> {\n    this.refill();\n    while (this.tokens < cost) {\n      const waitTime = ((cost - this.tokens) / this.refillRate) * 1000;\n      await new Promise(r => setTimeout(r, waitTime));\n      this.refill();\n    }\n    this.tokens -= cost;\n  }\n  \n  private refill(): void {\n    const now = Date.now();\n    const elapsed = (now - this.lastRefill) / 1000;\n    this.tokens = Math.min(this.capacity, this.tokens + elapsed * this.refillRate);\n    this.lastRefill = now;\n  }\n}\n```",
          "atomicFacts": [
            "Implements token bucket algorithm",
            "Supports variable cost acquisition",
            "Auto-refills based on time",
            "Async wait for tokens",
            "Caps at capacity"
          ],
          "sources": [
            "Rate limiting patterns"
          ]
        },
        "expectedWinner": "tie",
        "rationale": "Standard algorithm implementation"
      },
      "winner": "baseline",
      "systemScore": 1.5,
      "baselineScore": 5
    },
    {
      "sample": {
        "id": "cg-03",
        "category": "code_generation",
        "query": "Write a Python function to extract and validate citations from LLM output in the format [arxiv:ID] or [source:N].",
        "goldStandard": {
          "answer": "```python\nimport re\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass Citation:\n    type: str\n    id: str\n    valid: bool = True\n\ndef extract_citations(text: str, known_sources: List[str] = None) -> List[Citation]:\n    pattern = r'\\[(arxiv|perplexity|source):(\\w+)\\]'\n    citations = []\n    for match in re.finditer(pattern, text, re.IGNORECASE):\n        ctype, cid = match.groups()\n        valid = True\n        if known_sources and ctype.lower() == 'arxiv':\n            valid = cid in known_sources\n        citations.append(Citation(type=ctype.lower(), id=cid, valid=valid))\n    return citations\n```",
          "atomicFacts": [
            "Uses regex for extraction",
            "Supports multiple citation types",
            "Validates against known sources",
            "Returns structured Citation objects",
            "Case-insensitive matching"
          ],
          "sources": [
            "Citation parsing patterns"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Domain-specific code requires research context"
      },
      "winner": "baseline",
      "systemScore": 2,
      "baselineScore": 5
    },
    {
      "sample": {
        "id": "cg-04",
        "category": "code_generation",
        "query": "Write a TypeScript function to calculate Step-level F1 score for atomic proposition evaluation.",
        "goldStandard": {
          "answer": "```typescript\ninterface AtomicEvaluation {\n  precision: number;  // supported claims / total claims\n  recall: number;     // captured gold facts / total gold facts\n  f1: number;\n}\n\nfunction calculateStepF1(\n  systemClaims: string[],\n  supportedClaims: string[],\n  goldFacts: string[],\n  capturedFacts: string[]\n): AtomicEvaluation {\n  const precision = systemClaims.length > 0 \n    ? supportedClaims.length / systemClaims.length \n    : 0;\n  const recall = goldFacts.length > 0 \n    ? capturedFacts.length / goldFacts.length \n    : 0;\n  const f1 = (precision + recall) > 0 \n    ? 2 * (precision * recall) / (precision + recall) \n    : 0;\n  return { precision, recall, f1 };\n}\n```",
          "atomicFacts": [
            "Calculates precision from supported claims",
            "Calculates recall from captured facts",
            "Uses harmonic mean for F1",
            "Handles zero division",
            "Returns structured result"
          ],
          "sources": [
            "arxiv:1809.09600",
            "F1 calculation"
          ]
        },
        "expectedWinner": "mcp",
        "rationale": "Research-specific metric implementation"
      },
      "winner": "baseline",
      "systemScore": 2,
      "baselineScore": 4.5
    },
    {
      "sample": {
        "id": "cg-05",
        "category": "code_generation",
        "query": "Write a Python async function to batch embed documents while respecting rate limits.",
        "goldStandard": {
          "answer": "```python\nimport asyncio\nfrom typing import List\nimport openai\n\nasync def batch_embed(\n    texts: List[str],\n    model: str = 'text-embedding-3-small',\n    batch_size: int = 100,\n    rpm_limit: int = 3000,\n    tpm_limit: int = 1000000\n) -> List[List[float]]:\n    embeddings = []\n    rpm_delay = 60 / rpm_limit\n    \n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]\n        token_estimate = sum(len(t.split()) * 1.3 for t in batch)\n        \n        if token_estimate > tpm_limit / 60:\n            await asyncio.sleep(60)\n        \n        response = await openai.embeddings.create(input=batch, model=model)\n        embeddings.extend([e.embedding for e in response.data])\n        await asyncio.sleep(rpm_delay)\n    \n    return embeddings\n```",
          "atomicFacts": [
            "Async implementation",
            "Respects batch_size",
            "Calculates RPM delay",
            "Estimates token usage",
            "Handles TPM limit"
          ],
          "sources": [
            "OpenAI API docs",
            "Rate limiting"
          ]
        },
        "expectedWinner": "tie",
        "rationale": "API integration code"
      },
      "winner": "baseline",
      "systemScore": 1,
      "baselineScore": 5
    }
  ]
}